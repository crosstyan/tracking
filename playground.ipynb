{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T03:43:37.695427Z",
     "start_time": "2024-03-22T03:43:25.083457Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import Sequence, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"data.csv\")\n",
    "with_date = df.with_columns([pl.col(\"time\").str.to_datetime(\"%Y-%m-%d %H:%M:%S%.f\", strict=True).alias(\"time\")])\n",
    "with_date.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average interval for time\n",
    "t = with_date.select(\"time\").with_columns([pl.col(\"time\").diff().alias(\"diff\")])\n",
    "t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = df[\"x\"].to_numpy()\n",
    "ys = df[\"y\"].to_numpy()\n",
    "\n",
    "\n",
    "def plot_lines(xs: Sequence[Any], ys: Sequence[Any]):\n",
    "    assert len(xs) == len(ys)\n",
    "    c = len(xs)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, c))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, 100])\n",
    "    ax.set_ylim([0, 100])\n",
    "    # set the origin to the upper left\n",
    "    ax.invert_yaxis()\n",
    "    # plot line with color gradient\n",
    "    for i in range(c):\n",
    "        # plt.plot(xs[i - 1:i + 1], ys[i - 1:i + 1], c=colors[i - 1], linewidth=0.5)\n",
    "        plt.plot(xs[i - 1:i + 1], ys[i - 1:i + 1], c=colors[i - 1], marker=\"o\", markersize=5, linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_lines(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(xs, kde=True)\n",
    "sns.histplot(ys, kde=True)\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtremeFilter:\n",
    "    WINDOW_SIZE = 10\n",
    "    # shape should be (n, 2) where n <= WINDOW_SIZE\n",
    "    _window: NDArray = np.empty((0, 2))\n",
    "    # shape should be (m, 2) where m >= 0\n",
    "    _pts_inbound: NDArray = np.empty((0, 2))\n",
    "\n",
    "    def is_extreme(self, x: float | int, xs: NDArray, threshold: float = 10.0):\n",
    "        # use IQR\n",
    "        if len(xs) < self.WINDOW_SIZE:\n",
    "            return False\n",
    "        q1 = np.percentile(xs, 25)\n",
    "        q3 = np.percentile(xs, 75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0 and abs(x - q1) < threshold:\n",
    "            return False\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        return x < lower_bound or x > upper_bound\n",
    "\n",
    "    def append_point(self, x: float | int, y: float | int):\n",
    "        pt = np.array([[x, y]])\n",
    "        self._window = np.vstack([self._window, pt])\n",
    "        if self._window.shape[0] > self.WINDOW_SIZE:\n",
    "            self._window = self._window[-self.WINDOW_SIZE:]\n",
    "        xs = self._window[:, 0]\n",
    "        ys = self._window[:, 1]\n",
    "        if not any([self.is_extreme(x, xs), self.is_extreme(y, ys)]):\n",
    "            self._pts_inbound = np.vstack([self._pts_inbound, pt])\n",
    "\n",
    "    @property\n",
    "    def window(self):\n",
    "        return self._window\n",
    "\n",
    "    @property\n",
    "    def inbound(self):\n",
    "        return self._pts_inbound\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_data = df[\"x\"].to_numpy()\n",
    "ys_data = df[\"y\"].to_numpy()\n",
    "\n",
    "f = ExtremeFilter()\n",
    "for x, y in zip(xs_data, ys_data):\n",
    "    f.append_point(x, y)\n",
    "f.inbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(f.inbound[:, 0], f.inbound[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.array([xs, ys]).T\n",
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume time is constant (unit 1) \n",
    "# this is our velocity vector\n",
    "vel = np.diff(pts, axis=0)\n",
    "vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_mag = np.linalg.norm(vel, axis=1)\n",
    "vel_mag_norm = vel_mag / np.max(vel_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(vel_mag, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(vel_mag[vel_mag <= 10], kde=True)\n",
    "sns.histplot(vel_mag[vel_mag > 10], kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vel(x, y, u, v, norm=None, scale=1):\n",
    "    q = plt.quiver(x, y, u, v, norm, scale_units='xy', angles='xy', scale=scale, cmap=\"viridis\")\n",
    "    if norm is not None:\n",
    "        plt.colorbar(q, label=\"Velocity magnitude\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, 100])\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "\n",
    "plot_vel(pts[:-1, 0], pts[:-1, 1], vel[:, 0], vel[:, 1], vel_mag_norm, scale=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_v = np.hstack([pts[:-1], vel, vel_mag_norm.reshape(-1, 1)])\n",
    "BIG_THRES = 0.6\n",
    "big = pts_v[np.where(vel_mag_norm >= BIG_THRES)]\n",
    "display(np.shape(big))\n",
    "plot_vel(big[:, 0], big[:, 1], big[:, 2], big[:, 3], big[:, 4], scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.where(vel_mag_norm >= BIG_THRES)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = pts_v[np.where(vel_mag_norm < BIG_THRES)]\n",
    "display(np.shape(small))\n",
    "plot_vel(small[:, 0], small[:, 1], small[:, 2], small[:, 3], small[:, 4], scale=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pykalman import KalmanFilter\n",
    "# pykalman is broken \n",
    "# https://github.com/pykalman/pykalman/issues/106\n",
    "\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# https://github.com/rlabbe/filterpy/issues/223\n",
    "\n",
    "kf = KalmanFilter(dim_x=2, dim_z=2)\n",
    "kf.x = np.array([xs[0], ys[0]])\n",
    "kf.F = np.array([[1, 0],\n",
    "                 [0, 1]])\n",
    "kf.H = np.array([[1, 0],\n",
    "                 [0, 1]])\n",
    "kf.P *= 10\n",
    "kf.R = 10\n",
    "kf.Q = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "preds = np.empty((0, 2))\n",
    "inn = np.empty((0, 2))\n",
    "inn_norm = np.empty(0)\n",
    "for x, y in pts:\n",
    "    measurement = np.array([x, y])\n",
    "    kf.predict()\n",
    "    preds = np.vstack([preds, kf.x])\n",
    "    # innovation = measurement - H * predicted_state\n",
    "    # innovation is not always reliable\n",
    "    innovation = measurement - kf.H @ kf.x\n",
    "    inn_mag = np.linalg.norm(innovation)\n",
    "    inn_norm = np.append(inn_norm, inn_mag)\n",
    "    inn = np.vstack([inn, innovation])\n",
    "    INN_THRES = 35\n",
    "    SCALE_FACTOR = 100\n",
    "    kf.update(measurement)\n",
    "\n",
    "kf_xs = preds[:, 0]\n",
    "kf_ys = preds[:, 1]\n",
    "\n",
    "display(np.shape(preds))\n",
    "plot_lines(kf_xs, kf_ys)\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inn_df = pl.DataFrame({\"inn_x\": inn[:, 0], \"inn_y\": inn[:, 1], \"inn_mag\": inn_norm})\n",
    "inn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hysteresis\n",
    "Implement hysteresis into the decision-making process. This means using two thresholds instead of one: a higher threshold to trigger a change, and a lower threshold to return to the previous state. This can prevent the system from toggling back and forth near the boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, x_vel, y_vel\n",
    "PARTICLE_COUNT = 3000\n",
    "SPEED = 2\n",
    "particles = np.empty((PARTICLE_COUNT, 4), dtype=float)\n",
    "particles[:, 0] = np.random.uniform(0, 100, PARTICLE_COUNT)\n",
    "particles[:, 1] = np.random.uniform(0, 100, PARTICLE_COUNT)\n",
    "particles[:, 2] = np.random.uniform(-SPEED, SPEED, PARTICLE_COUNT)\n",
    "particles[:, 3] = np.random.uniform(-SPEED, SPEED, PARTICLE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(particles[:, 0], particles[:, 1], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(particles: NDArray, dt: float = 1.0, speed_std: float = 1.0):\n",
    "    # add noise\n",
    "    particles[:, 0] += (particles[:, 2] + np.random.normal(0, speed_std, PARTICLE_COUNT)) * dt\n",
    "    particles[:, 1] += (particles[:, 3] + np.random.normal(0, speed_std, PARTICLE_COUNT)) * dt\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We do the same with our particles. Each particle has a position and a weight which estimates how well it matches the\n",
    "measurement. Normalizing the weights, so they sum to one turns them into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def update(particles: NDArray, weights: NDArray, x: float, y: float) -> NDArray:\n",
    "    MEASUREMENT_NOISE = 10\n",
    "    distance = np.linalg.norm(particles[:, :2] - np.array([x, y]), axis=1)\n",
    "    weights *= scipy.stats.norm(0, MEASUREMENT_NOISE).pdf(distance)\n",
    "    weights += 1.e-300  # avoid round-off to zero\n",
    "    weights /= sum(weights)  # normalize\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We don't resample at every epoch. For example, if you received no new measurements you have not received any information from which the resample can benefit. We can determine when to resample by using something called the *effective N*, which approximately measures the number of particles which meaningfully contribute to the probability distribution. The equation for this is\n",
    "\n",
    "$$\\hat{N}_\\text{eff} = \\frac{1}{\\sum w^2}$$\n",
    "\n",
    "and we can implement this in Python with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "def simple_resample(particles, weights):\n",
    "    N = len(particles)\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1.  # avoid round-off error\n",
    "    indexes = np.searchsorted(cumulative_sum, random(N))\n",
    "\n",
    "    # resample according to indexes\n",
    "    particles[:] = particles[indexes]\n",
    "    weights.fill(1.0 / N)\n",
    "\n",
    "\n",
    "def neff(weights):\n",
    "    return 1. / np.sum(np.square(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If $\\hat{N}_\\text{eff}$ falls below some threshold it is time to resample. A useful starting point is $N/2$, but this varies by problem. It is also possible for $\\hat{N}_\\text{eff} = N$, which means the particle set has collapsed to one point (each has equal weight). It may not be theoretically pure, but if that happens I create a new distribution of particles in the hopes of generating particles with more diversity. If this happens to you often, you may need to increase the number of particles, or otherwise adjust your filter. We will talk more of this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights.resize(len(particles))\n",
    "    weights.fill(1.0 / len(weights))\n",
    "\n",
    "\n",
    "# https://filterpy.readthedocs.io/en/latest/monte_carlo/resampling.html\n",
    "from filterpy.monte_carlo import systematic_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones(PARTICLE_COUNT) / PARTICLE_COUNT\n",
    "alpha = 0.2\n",
    "if PARTICLE_COUNT > 5000:\n",
    "    alpha *= np.sqrt(5000) / np.sqrt(PARTICLE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "    pos = particles[:, 0:2]\n",
    "    mean = np.average(pos, weights=weights, axis=0)\n",
    "    var = np.average((pos - mean) ** 2, weights=weights, axis=0)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 152\n",
    "from time import time\n",
    "start = time()\n",
    "particles = predict(particles)\n",
    "weights = update(particles, weights, xs[c], ys[c])\n",
    "plt.scatter(particles[:, 0], particles[:, 1], s=1, alpha=alpha)\n",
    "plt.scatter(xs[c], ys[c], color='r')\n",
    "axis = plt.gca()\n",
    "axis.set_xlim([0, 100])\n",
    "axis.set_ylim([0, 100])\n",
    "axis.invert_yaxis()\n",
    "# resample if too few effective particles\n",
    "N = PARTICLE_COUNT\n",
    "if neff(weights) < N / 2:\n",
    "    indexes = systematic_resample(weights)\n",
    "    resample_from_index(particles, weights, indexes)\n",
    "    assert np.allclose(weights, 1 / N)\n",
    "mean, var = estimate(particles, weights)\n",
    "display(time() - start)\n",
    "plt.scatter(mean[0], mean[1], color='g', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weights, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
    "                                               ConstantVelocity\n",
    "from stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState\n",
    "from stonesoup.types.detection import TrueDetection\n",
    "from stonesoup.types.detection import Clutter\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.05),\n",
    "                                                          ConstantVelocity(0.05)])\n",
    "measurement_model = LinearGaussian(ndim_state=4, mapping=(0, 2), noise_covar=np.diag([0.75, 0.75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.predictor.kalman import KalmanPredictor\n",
    "from stonesoup.updater.kalman import KalmanUpdater\n",
    "\n",
    "predictor = KalmanPredictor(transition_model)\n",
    "\n",
    "updater = KalmanUpdater(measurement_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.hypothesiser.distance import DistanceHypothesiser\n",
    "from stonesoup.measures import Mahalanobis\n",
    "from stonesoup.dataassociator.neighbour import GlobalNearestNeighbour\n",
    "\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "\n",
    "data_associator = GlobalNearestNeighbour(hypothesiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.deleter.error import CovarianceBasedDeleter\n",
    "from stonesoup.types.state import GaussianState\n",
    "from stonesoup.initiator.simple import MultiMeasurementInitiator\n",
    "\n",
    "deleter = CovarianceBasedDeleter(covar_trace_thresh=4)\n",
    "\n",
    "initiator = MultiMeasurementInitiator(\n",
    "    prior_state=GaussianState([[0], [0], [0], [0]], np.diag([0, 1, 0, 1])),\n",
    "    measurement_model=measurement_model,\n",
    "    deleter=deleter,\n",
    "    data_associator=data_associator,\n",
    "    updater=updater,\n",
    "    min_points=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.types.detection import Detection\n",
    "detections = np.array([xs, ys])\n",
    "# for n, v in enumerate(detections.T):\n",
    "#     measurement = Detection(v)\n",
    "#     print(v)\n",
    "\n",
    "\n",
    "tracks, all_tracks = set(), set()\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "all_measurements = [set([Detection(v, timestamp=start_time + timedelta(seconds=n))]) for n, v in enumerate(detections.T)]\n",
    "time_steps = []\n",
    "for n, measurements in enumerate(all_measurements):\n",
    "    time_steps.append(start_time + timedelta(seconds=n))\n",
    "    # Calculate all hypothesis pairs and associate the elements in the best subset to the tracks.\n",
    "    hypotheses = data_associator.associate(tracks,\n",
    "                                           measurements,\n",
    "                                           start_time + timedelta(seconds=n))\n",
    "    associated_measurements = set()\n",
    "    for track in tracks:\n",
    "        hypothesis = hypotheses[track]\n",
    "        if hypothesis.measurement:\n",
    "            post = updater.update(hypothesis)\n",
    "            track.append(post)\n",
    "            associated_measurements.add(hypothesis.measurement)\n",
    "        else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "            track.append(hypothesis.prediction)\n",
    "\n",
    "    # Carry out deletion and initiation\n",
    "    tracks -= deleter.delete_tracks(tracks)\n",
    "    tracks |= initiator.initiate(measurements - associated_measurements,\n",
    "                                 start_time + timedelta(seconds=n))\n",
    "    all_tracks |= tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_measurements\n",
    "len(all_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.plotter import AnimatedPlotterly\n",
    "plotter = AnimatedPlotterly(time_steps, tail_length=0.3)\n",
    "plotter.plot_tracks(all_tracks, [0, 2], uncertainty=True)\n",
    "# plotter.fig.update_layout(xaxis=dict(range=[0, 100]), yaxis=dict(range=[0, 100]))\n",
    "plotter.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "times = list(range(len(xs)))\n",
    "# Get a list of colors from the 'viridis' colormap in Matplotlib\n",
    "viridis_cmap = plt.cm.get_cmap('viridis', len(times))\n",
    "viridis_colors = [viridis_cmap(i) for i in range(len(times))]\n",
    "\n",
    "# Convert RGBA colors to RGB format as a hex string for Plotly\n",
    "viridis_hex = ['rgba(' + ','.join([f'{int(c*255)}' for c in color]) + ')' for color in viridis_colors]\n",
    "\n",
    "for i, t in enumerate(times):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xs[:i+1], \n",
    "        y=ys[:i+1], \n",
    "        marker=dict(color=viridis_hex[:i+1]),\n",
    "        mode='markers', \n",
    "        visible=False))\n",
    "\n",
    "fig.data[0].visible = True\n",
    "\n",
    "fig.update_layout(xaxis=dict(range=[0, 100]), yaxis=dict(range=[0, 100]))\n",
    "steps = []\n",
    "for i, t in enumerate(times):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)},\n",
    "              {\"title\": f\"Time: {t}\"}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "slider = [dict(\n",
    "  active=0,\n",
    "    currentvalue={\"prefix\": \"Time: \"},\n",
    "    pad={\"t\": 50}, # top padding\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(sliders=slider)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
